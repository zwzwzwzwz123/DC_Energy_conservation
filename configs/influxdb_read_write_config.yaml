# influxdb_read_write_config.yaml - InfluxDB 读写配置
# 本配置文件定义了数据读取策略和写入策略

# ==================== 读取配置 ====================
read:
  # 全局默认配置（所有客户端共享）
  default:
    # 数据量选择模式: "time_range" 或 "last_n_points"
    mode: "time_range"

    # time_range 模式参数
    time_range:
      duration: 1  # 时间范围(数值)
      unit: "h"    # 时间单位: "h"(小时), "m"(分钟), "d"(天)

    # last_n_points 模式参数
    last_n_points:
      count: 100   # 读取最近N条数据

    # 默认 field_key
    default_field_key: "value"

    # 时间排序方式: "asc"(升序) 或 "desc"(降序)
    # asc: 从旧到新排序（升序，时间戳小的在前面）, desc: 从新到旧排序（降序，时间戳大的在前面）
    time_order: "desc"

    # 是否包含不可用设备/机房的数据（默认 false）
    # true: 包含不可用设备/机房的数据（用于诊断）, false: 只包含可用设备/机房的数据
    include_unavailable: false

    # Tag Set 过滤配置（可选）
    # 当配置了 tag_filters 时，在查询中添加相应的 WHERE 条件
    # 格式: tag_key: tag_value 或 tag_key: [value1, value2] (多值匹配)
    tag_filters: {}

  # ==================== dc_status_data_client 客户端的读取配置 ====================
  # 用于从 iot_origin_database 读取数据中心状态数据
  dc_status_data_client:
    # 客户端级别的默认配置（可选，覆盖全局 default）
    default:
      mode: "time_range"
      time_range:
        duration: 1
        unit: "h"

    # 配置项 1: 读取数据中心最新状态（所有可观测数据）
    datacenter_latest_status:
      # 必填：读取方法类型
      # 可选值: read_all_observable_data / read_room_data / read_device_data / read_specific_uids
      read_method: "read_all_observable_data"

      # 可选：是否包含不可用设备/机房的数据（默认 false）
      include_unavailable: false

    # 配置项 2: 读取关键设备数据
    critical_devices_data:
      read_method: "read_device_data"

      # 必填（当 read_method 为 read_device_data 时）：指定要读取的设备 UID 列表
      device_uids: [ "AC_A1_001", "AC_A1_002" ]

      # 覆盖默认配置：读取最近 2 小时的数据
      mode: "time_range"
      time_range:
        duration: 2
        unit: "h"

    # 配置项 3: 读取特定测点数据
    specific_sensors_data:
      read_method: "read_specific_uids"

      # 必填（当 read_method 为 read_specific_uids 时）：指定要读取的属性 UID 列表
      specific_uids: [ "ac_a1_001_supply_temp", "ac_a1_001_return_temp","comp_a1_001_power" ]

      # 覆盖默认配置：使用最近 100 条数据
      mode: "last_n_points"
      last_n_points:
        count: 100

    # 配置项 4: 读取多个机房的历史数据
    multi_room_history:
      read_method: "read_room_data"
      room_uids: [ "CR_A1", "CR_B2" ]
      include_unavailable: false

      # 读取最近 24 小时的数据
      mode: "time_range"
      time_range:
        duration: 24
        unit: "h"

    # 配置项 5: 诊断模式（包含不可用设备）
    diagnostic_all_data:
      read_method: "read_all_observable_data"
      include_unavailable: true  # 包含不可用设备的数据，用于诊断

      mode: "time_range"
      time_range:
        duration: 1
        unit: "h"

  # ==================== prediction_data_client 客户端的读取配置 ====================
  # 用于从 iot_origin_prediction 读取预测数据
  prediction_data_client:
    # 客户端级别的默认配置
    default:
      mode: "last_n_points"
      last_n_points:
        count: 50

    # 配置项 1: 读取最新预测结果（指定机房）
    latest_predictions:
      read_method: "read_room_data"

      # 必填（当 read_method 为 read_room_data 时）：指定要读取的机房 UID 列表
      room_uids: [ "CR_A1" ]

      include_unavailable: false

      # 覆盖默认配置：使用最近 100 条数据
      mode: "last_n_points"
      last_n_points:
        count: 100

    # 配置项 2: 读取预测历史数据
    prediction_history:
      read_method: "read_specific_uids"

      # 指定要读取的预测数据 UID 列表
      # 注意：预测数据的 measurement 命名规则为 {room_uid}_temp_pred_{horizon}
      specific_uids: [ "CR_A1_temp_pred_1h", "CR_A1_temp_pred_6h" ]

      # 读取最近 6 小时的预测历史
      mode: "time_range"
      time_range:
        duration: 6
        unit: "h"

    # 配置项 3: 读取特定时间范围的预测数据（带 Tag 过滤）
    filtered_predictions:
      read_method: "read_all_observable_data"

      # Tag Set 过滤配置
      tag_filters:
        horizon: "15mins"  # 只读取 15 分钟预测数据

      mode: "time_range"
      time_range:
        duration: 2
        unit: "h"

    # 配置项 4: 读取多个 horizon 的预测数据
    multi_horizon_predictions:
      read_method: "read_specific_uids"
      specific_uids: [ "ai_prediction_results" ]

      # 多值 Tag 过滤
      tag_filters:
        horizon: ["15mins", "1h", "6h"]  # 读取多个时间范围的预测

      mode: "last_n_points"
      last_n_points:
        count: 200


# ==================== 写入配置 ====================
write:
  # 全局默认配置（所有客户端共享）
  default:
    batch_size: 100
    retry_times: 3
    retry_interval: 3

  # ==================== prediction_data_client 客户端的写入配置 ====================
  # 用于向 iot_origin_prediction 写入预测数据
  prediction_data_client:
    # 客户端级别的默认配置（可选，覆盖全局 default）
    default:
      batch_size: 100
      retry_times: 3
      retry_interval: 3

    # 配置项 1: 按测点分离存储预测数据
    prediction_by_uid:
      # 写入模式：separate_by_uid（按测点分离） 或 unified_format（统一格式）
      write_mode: "separate_by_uid"

      # 数据点构建配置
      measurement_template: "{uid}"  # 使用测点uid作为measurement
      tag_set:
        horizon: "{horizon}"  # 预测时间范围作为tag
      field_set:
        value: "{value}"  # 预测数值作为field

      # 覆盖默认配置
      batch_size: 50

    # 配置项 2: 统一格式存储预测数据
    prediction_unified:
      write_mode: "unified_format"

      # 数据点构建配置
      measurement_name: "ai_prediction_results"  # 固定measurement名称
      tag_set:
        horizon: "{horizon}"
        uid: "{uid}"
      field_set:
        content: "{json_content}"  # JSON格式内容



  # ==================== optimization_data_client 客户端的写入配置 ====================
  # 用于向 iot_origin_optimization 写入优化控制指令
  optimization_data_client:
    # 客户端级别的默认配置
    default:
      batch_size: 50
      retry_times: 3
      retry_interval: 2

    # 配置项 1: 按测点分离存储优化指令
    optimization_by_uid:
      write_mode: "separate_by_uid"

      # 数据点构建配置
      measurement_template: "{uid}"  # 使用测点uid作为measurement
      tag_set:
        is_auto_execute: "{is_auto_execute}"  # 是否自动执行
      field_set:
        value: "{value}"  # 控制数值

    # 配置项 2: 统一格式存储优化指令
    optimization_unified:
      write_mode: "unified_format"

      # 数据点构建配置
      measurement_name: "ai_optimization_commands"  # 固定measurement名称
      tag_set:
        is_auto_execute: "{is_auto_execute}"
        uid: "{uid}"
      field_set:
        content: "{json_content}"  # JSON格式内容



# ==================== 查询优化配置 ====================
query_optimization:

  # 是否启用并行查询(对于大量 uid 的情况)
  enable_parallel_query: true

  # 并行查询的线程数
  parallel_threads: 4

  # 单次查询的最大 uid 数量(超过则分批查询)
  max_uids_per_query: 200
